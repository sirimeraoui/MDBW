{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "101ebec0",
   "metadata": {},
   "source": [
    "# Task 4: Privacy and Fairness\n",
    "\n",
    "In this section the goal is to train a classifier that is both private and fair and compare the results with previous ones. \n",
    "\n",
    "1. We apply **Local Differential Privacy** to the sensitive attributes (`Sex` and `Age`).\n",
    "2. We try to reduce bias by **Reweighing** on this noisy data.\n",
    "3. Finally, we check the model using the real data to see if the fairness mitigation was effective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
    "import joblib\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Starting Task 4: Private + Fair Classifier...\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. Load & Basic Prep\n",
    "# ==========================================\n",
    "\n",
    "cols = [\n",
    "    'Age', 'Workclass', 'fnlwgt', 'Education', 'Education-Num',\n",
    "    'Marital Status', 'Occupation', 'Relationship', 'Race', 'Sex',\n",
    "    'Capital Gain', 'Capital Loss', 'Hours per week', 'Country', 'Target'\n",
    "]\n",
    "\n",
    "# Try loading from typical filenames\n",
    "try:\n",
    "    df = pd.read_csv('adult.data', header=None, names=cols, skipinitialspace=True)\n",
    "except FileNotFoundError:\n",
    "    df = pd.read_csv('adult.data.csv', header=None, names=cols, skipinitialspace=True)\n",
    "\n",
    "# Clean target\n",
    "df['Target'] = df['Target'].str.strip()\n",
    "df['target_binary'] = (df['Target'] == '>50K').astype(int)\n",
    "\n",
    "print(f\"Data loaded: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d188e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Task 4: Private + Fair Classifier...\n",
      "Data loaded: (32561, 16)\n",
      "Applying LDP noise (epsilon=1.0)...\n",
      "Features ready. Count: 101\n",
      "Calculating weights via Reweighing (on noisy data)...\n",
      "Training Random Forest with privacy & fairness weights...\n",
      "Done. Model trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['test_data_q4.joblib']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. Local Differential Privacy\n",
    "# ==========================================\n",
    "\n",
    "def randomized_response(values, epsilon, categories):\n",
    "    \"\"\"\n",
    "    Applies k-ary randomized response.\n",
    "    \"\"\"\n",
    "    values = pd.Series(values)\n",
    "    n = len(values)\n",
    "    k = len(categories)\n",
    "    \n",
    "    # Probability of preserving the true value\n",
    "    p = np.exp(epsilon) / (np.exp(epsilon) + k - 1)\n",
    "    \n",
    "    noisy_vals = []\n",
    "    for v in values:\n",
    "        # Handle missing or unknown categories by picking random\n",
    "        if v not in categories or pd.isna(v):\n",
    "            noisy_vals.append(np.random.choice(categories))\n",
    "            continue\n",
    "            \n",
    "        # Flip coin based on epsilon\n",
    "        if np.random.rand() < p:\n",
    "            noisy_vals.append(v)\n",
    "        else:\n",
    "            # Pick from the other categories\n",
    "            others = [c for c in categories if c != v]\n",
    "            noisy_vals.append(np.random.choice(others))\n",
    "            \n",
    "    return np.array(noisy_vals)\n",
    "\n",
    "# Create Age bins (needed for the LDP step, same as Q3)\n",
    "bins = [0, 30, 40, 50, 100]\n",
    "labels = [\"<=30\", \"31-40\", \"41-50\", \"51+\"]\n",
    "df[\"AgeGroup\"] = pd.cut(df[\"Age\"], bins=bins, labels=labels, right=True, include_lowest=True)\n",
    "\n",
    "# LDP Params\n",
    "epsilon = 1.0\n",
    "age_cats = labels\n",
    "sex_cats = df[\"Sex\"].unique().tolist()\n",
    "\n",
    "print(f\"Applying LDP noise (epsilon={epsilon})...\")\n",
    "\n",
    "# Create the noisy columns\n",
    "df[\"AgeGroup_noisy\"] = randomized_response(df[\"AgeGroup\"], epsilon, age_cats)\n",
    "df[\"Sex_noisy\"] = randomized_response(df[\"Sex\"], epsilon, sex_cats)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Reconstruct Private Features\n",
    "# ==========================================\n",
    "\n",
    "# Map noisy categorical data back to numeric/binary for the model\n",
    "\n",
    "# 1. Private Sex (1 = Noisy Male)\n",
    "df[\"Sex_priv_Male\"] = (df[\"Sex_noisy\"] == \"Male\").astype(int)\n",
    "\n",
    "# 2. Private Age (using midpoints of the bins)\n",
    "age_map = {\"<=30\": 25, \"31-40\": 35, \"41-50\": 45, \"51+\": 60}\n",
    "df[\"Age_priv\"] = df[\"AgeGroup_noisy\"].map(age_map).astype(float)\n",
    "\n",
    "# Binarize Private Age for AIF360 logic\n",
    "# We use >30 as the cutoff for 'privileged' to keep it simple based on our bins\n",
    "df['age_priv_binary'] = (df['Age_priv'] > 30).astype(int)\n",
    "\n",
    "# ==========================================\n",
    "# 4. Feature Matrix Setup\n",
    "# ==========================================\n",
    "\n",
    "# Drop original sensitive cols, intermediates, and targets\n",
    "cols_to_drop = ['Age', 'Sex', 'AgeGroup', 'AgeGroup_noisy', 'Sex_noisy', 'Target', 'target_binary']\n",
    "cat_features = ['Workclass', 'Education', 'Marital Status', 'Occupation', 'Relationship', 'Race', 'Country']\n",
    "\n",
    "# One-hot encoding\n",
    "df_encoded = pd.get_dummies(\n",
    "    df.drop(columns=cols_to_drop + ['age_priv_binary', 'Age_priv', 'Sex_priv_Male']), \n",
    "    columns=cat_features, \n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# Re-attach the *private* features\n",
    "X = df_encoded.copy()\n",
    "X['Age_priv'] = df['Age_priv']\n",
    "X['Sex_priv_Male'] = df['Sex_priv_Male']\n",
    "# Aux binary col for AIF360\n",
    "X['age_priv_binary'] = df['age_priv_binary']\n",
    "\n",
    "y = df['target_binary']\n",
    "\n",
    "print(f\"Features ready. Count: {X.shape[1]}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. Fairness Mitigation (Reweighing)\n",
    "# ==========================================\n",
    "\n",
    "# Split first to prevent leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Setup AIF360 dataset (Training data only)\n",
    "df_train_aif = X_train.copy()\n",
    "df_train_aif['Target'] = y_train.values\n",
    "\n",
    "# Define groups based on NOISY attributes\n",
    "# Privileged: Noisy Male & Noisy Older\n",
    "priv_groups = [{'Sex_priv_Male': 1, 'age_priv_binary': 1}]\n",
    "unpriv_groups = [{'Sex_priv_Male': 0, 'age_priv_binary': 0}]\n",
    "\n",
    "dataset_aif = BinaryLabelDataset(\n",
    "    df=df_train_aif,\n",
    "    label_names=['Target'],\n",
    "    protected_attribute_names=['Sex_priv_Male', 'age_priv_binary'],\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0\n",
    ")\n",
    "\n",
    "print(\"Calculating weights via Reweighing (on noisy data)...\")\n",
    "RW = Reweighing(unprivileged_groups=unpriv_groups, privileged_groups=priv_groups)\n",
    "RW.fit(dataset_aif)\n",
    "dataset_transf = RW.transform(dataset_aif)\n",
    "\n",
    "# Get the new weights\n",
    "sample_weights = dataset_transf.instance_weights\n",
    "\n",
    "# ==========================================\n",
    "# 6. Train Model\n",
    "# ==========================================\n",
    "\n",
    "print(\"Training Random Forest with privacy & fairness weights...\")\n",
    "\n",
    "clf_priv_fair = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit using the private features and the calculated weights\n",
    "clf_priv_fair.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "print(\"Done. Model trained.\")\n",
    "\n",
    "# Save artifacts for later use\n",
    "joblib.dump(clf_priv_fair, 'private_fair_classifier.joblib')\n",
    "joblib.dump((X_test, y_test), 'test_data_q4.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "157a6ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Starting Fairness Audit using Real Data...\n",
      "--------------------------------------------------\n",
      "AUDIT RESULTS (Private + Fair Classifier)\n",
      "--------------------------------------------------\n",
      "Accuracy:                0.8454\n",
      "Disparate Impact:        0.1684 (Ideal: 1.0)\n",
      "Statistical Parity Diff: -0.2969 (Ideal: 0.0)\n",
      "Equal Opportunity Diff:  -0.0187 (Ideal: 0.0)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "print(\">>> Starting Fairness Audit using Real Data...\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. Prepare Audit Dataframe\n",
    "# ==========================================\n",
    "\n",
    "# Pull original rows corresponding to the test set to get real sensitive attrs\n",
    "df_eval = df.loc[X_test.index].copy()\n",
    "\n",
    "# Add Model Predictions (based on private features) and True Targets\n",
    "df_eval['y_pred'] = clf_priv_fair.predict(X_test)\n",
    "df_eval['y_true'] = y_test.values\n",
    "\n",
    "# Reconstruct REAL binary sensitive attributes (Ground Truth, no noise)\n",
    "df_eval['sex_real'] = (df_eval['Sex'] == 'Male').astype(int)\n",
    "\n",
    "# Real Age Binary (using the original median)\n",
    "median_age = df['Age'].median()\n",
    "df_eval['age_real'] = (df['Age'] > median_age).astype(int)\n",
    "\n",
    "# Subset only numeric columns for AIF360 to avoid ValueError\n",
    "audit_cols = ['y_true', 'y_pred', 'sex_real', 'age_real']\n",
    "df_aif = df_eval[audit_cols].copy()\n",
    "\n",
    "# ==========================================\n",
    "# 2. Setup AIF360 Datasets\n",
    "# ==========================================\n",
    "\n",
    "# Define groups based on REAL attributes\n",
    "# Privileged: Real Men & Real Older > Median\n",
    "priv_groups = [{'sex_real': 1, 'age_real': 1}]\n",
    "unpriv_groups = [{'sex_real': 0, 'age_real': 0}]\n",
    "\n",
    "# Ground Truth Dataset\n",
    "ds_true = BinaryLabelDataset(\n",
    "    df=df_aif.drop(columns=['y_pred']),\n",
    "    label_names=['y_true'],\n",
    "    protected_attribute_names=['sex_real', 'age_real'],\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0\n",
    ")\n",
    "\n",
    "# Prediction Dataset (swap target col with prediction col)\n",
    "df_pred_temp = df_aif.copy()\n",
    "df_pred_temp['y_true'] = df_pred_temp['y_pred'] # Overwrite label with prediction\n",
    "df_pred_temp.drop(columns=['y_pred'], inplace=True)\n",
    "\n",
    "ds_pred = BinaryLabelDataset(\n",
    "    df=df_pred_temp,\n",
    "    label_names=['y_true'],\n",
    "    protected_attribute_names=['sex_real', 'age_real'],\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 3. Compute & Report Metrics\n",
    "# ==========================================\n",
    "\n",
    "metric = ClassificationMetric(\n",
    "    ds_true, \n",
    "    ds_pred,\n",
    "    unprivileged_groups=unpriv_groups,\n",
    "    privileged_groups=priv_groups\n",
    ")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"AUDIT RESULTS (Private + Fair Classifier)\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Accuracy:                {metric.accuracy():.4f}\")\n",
    "print(f\"Disparate Impact:        {metric.disparate_impact():.4f} (Ideal: 1.0)\")\n",
    "print(f\"Statistical Parity Diff: {metric.statistical_parity_difference():.4f} (Ideal: 0.0)\")\n",
    "print(f\"Equal Opportunity Diff:  {metric.equal_opportunity_difference():.4f} (Ideal: 0.0)\")\n",
    "print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
